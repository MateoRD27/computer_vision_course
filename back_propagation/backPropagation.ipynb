{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c4f5f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 1/8000 - Loss: 0.716595\n",
      "Época 800/8000 - Loss: 0.007541\n",
      "Época 1600/8000 - Loss: 0.003307\n",
      "Época 2400/8000 - Loss: 0.002112\n",
      "Época 3200/8000 - Loss: 0.001551\n",
      "Época 4000/8000 - Loss: 0.001225\n",
      "Época 4800/8000 - Loss: 0.001012\n",
      "Época 5600/8000 - Loss: 0.000862\n",
      "Época 6400/8000 - Loss: 0.000751\n",
      "Época 7200/8000 - Loss: 0.000665\n",
      "Época 8000/8000 - Loss: 0.000597\n",
      "XOR - Predicciones: [0 1 1 0]\n",
      "XOR - Probabilidades: [[4.24916120e-05]\n",
      " [9.99333402e-01]\n",
      " [9.99325033e-01]\n",
      " [1.00203049e-03]]\n",
      "Entrada: [[1 1]]\n",
      "Clase predicha: [0]\n",
      "Probabilidad: [[0.00100203]]\n",
      "Época 1/300 - Loss: 1.305247\n",
      "Época 30/300 - Loss: 1.098291\n",
      "Época 60/300 - Loss: 1.077576\n",
      "Época 90/300 - Loss: 1.068970\n",
      "Época 120/300 - Loss: 1.063496\n",
      "Época 150/300 - Loss: 1.059181\n",
      "Época 180/300 - Loss: 1.055707\n",
      "Época 210/300 - Loss: 1.052893\n",
      "Época 240/300 - Loss: 1.050453\n",
      "Época 270/300 - Loss: 1.047897\n",
      "Época 300/300 - Loss: 1.045593\n",
      "Multiclase - primeros 10 predicciones: [0 2 2 2 2 2 2 1 2 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Proposito: Implementacion didactica y practica de forward + backprop (SGD y momentum),\n",
    "# con soporte para cualquier numero de capas, activaciones 'sigmoide','tanh','relu',\n",
    "# salida 'binario' (sigmoide) o 'multiclase' (softmax). Todo el codigo esta comentado\n",
    "\n",
    "# Importar NumPy para operaciones numericas y matriciales\n",
    "import numpy as np  # biblioteca principal para vectores y matrices\n",
    "\n",
    "# -------------------------------\n",
    "# Funciones de activacion y derivadas (cada linea comentada)\n",
    "# -------------------------------\n",
    "\n",
    "def sigmoide(z):\n",
    "    # recibe z (array) y devuelve la funcion sigmoide elemento a elemento\n",
    "    return 1.0 / (1.0 + np.exp(-z))  # 1/(1+e^-z)\n",
    "\n",
    "\n",
    "def d_sigmoide(z):\n",
    "    # calcula la derivada de la sigmoide usando la identidad s'(z)=s(z)*(1-s(z))\n",
    "    s = sigmoide(z)            # s = sigmoide(z)\n",
    "    return s * (1.0 - s)       # devolver s*(1-s)\n",
    "\n",
    "\n",
    "def tanh_act(z):\n",
    "    # funcion tangente hiperbolica (tanh) aplicada elemento a elemento\n",
    "    return np.tanh(z)          # numpy implementa tanh vectorizada\n",
    "\n",
    "\n",
    "def d_tanh(z):\n",
    "    # derivada de tanh: 1 - tanh(z)^2\n",
    "    t = np.tanh(z)            # t = tanh(z)\n",
    "    return 1.0 - t * t        # devolver 1 - t^2\n",
    "\n",
    "\n",
    "def relu(z):\n",
    "    # ReLU: devuelve z si z>0, si no devuelve 0\n",
    "    return np.maximum(0.0, z) # operacion vectorizada, mantiene forma de z\n",
    "\n",
    "\n",
    "def d_relu(z):\n",
    "    # derivada de ReLU: 1 para z>0, 0 para z<=0\n",
    "    return (z > 0).astype(float)  # booleanos convertidos a float (1.0/0.0)\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    # softmax por filas (cada fila corresponde a un ejemplo)\n",
    "    # restamos el maximo por fila para estabilidad numerica\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # exponentes estables\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)   # normalizar por fila\n",
    "\n",
    "# Diccionario que asocia nombres con (func, deriv)\n",
    "_ACT_FUNCS = {\n",
    "    'sigmoide': (sigmoide, d_sigmoide),  # para capas ocultas\n",
    "    'tanh': (tanh_act, d_tanh),\n",
    "    'relu': (relu, d_relu)\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# Inicializacion de parametros (pesos y biases)\n",
    "# -------------------------------\n",
    "\n",
    "def inicializar_parametros(capas, metodo_init='xavier', seed=None):\n",
    "    # capas: lista de enteros [n_entrada, n_h1, ..., n_salida]\n",
    "    # metodo_init: 'xavier' o 'he' (usar 'he' con ReLU)\n",
    "    # seed: semilla aleatoria para reproducibilidad\n",
    "\n",
    "    # Si el usuario proporciona semilla, fijarla en numpy\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)  # fija la generacion aleatoria\n",
    "\n",
    "    parametros = {}  # dict que guardara W1,b1,W2,b2,...\n",
    "\n",
    "    # iterar sobre las transiciones de capa anterior -> capa actual\n",
    "    for i in range(1, len(capas)):\n",
    "        n_prev = capas[i-1]  # neuronas en capa anterior\n",
    "        n_curr = capas[i]    # neuronas en capa actual\n",
    "\n",
    "        # elegir esquema de inicializacion\n",
    "        if metodo_init == 'he':\n",
    "            # He: N(0, 2/n_prev) recomendado para ReLU\n",
    "            parametros[f'W{i}'] = np.random.randn(n_prev, n_curr) * np.sqrt(2.0 / max(1, n_prev))\n",
    "        else:\n",
    "            # Xavier/Glorot aproximado: N(0, 1/n_prev), bueno para sigmoide/tanh\n",
    "            parametros[f'W{i}'] = np.random.randn(n_prev, n_curr) * np.sqrt(1.0 / max(1, n_prev))\n",
    "\n",
    "        # biases inicializados en cero (1 x n_curr)\n",
    "        parametros[f'b{i}'] = np.zeros((1, n_curr))\n",
    "\n",
    "    # devolver diccionario con todos los parametros inicializados\n",
    "    return parametros\n",
    "\n",
    "# -------------------------------\n",
    "# Forward pass (propagacion hacia adelante)\n",
    "# -------------------------------\n",
    "\n",
    "def forward(X, parametros, activaciones_ocultas='relu', problema='multiclase'):\n",
    "    # X: (m, n_entradas) matriz de ejemplos\n",
    "    # parametros: diccionario de pesos y biases\n",
    "    # activaciones_ocultas: string o lista con nombre(s) de activacion para ocultas\n",
    "    # problema: 'multiclase' o 'binario'\n",
    "\n",
    "    cache = {}                 # cache para almacenar A0, Z1, A1, ..., ZL, AL\n",
    "    cache['A0'] = X.copy()     # A0 es la entrada; copiamos para seguridad\n",
    "\n",
    "    L = len(parametros) // 2   # numero de capas (W,b pares)\n",
    "\n",
    "    # normalizar activaciones a una lista con longitud L-1 (solo para capas ocultas)\n",
    "    if isinstance(activaciones_ocultas, str):\n",
    "        activaciones = [activaciones_ocultas] * max(0, L-1)\n",
    "    else:\n",
    "        activaciones = list(activaciones_ocultas)\n",
    "\n",
    "    # si el usuario dio una lista mas corta o vacia, repetir/rellenar con la primera\n",
    "    if len(activaciones) != max(0, L-1):\n",
    "        if len(activaciones) == 0:\n",
    "            activaciones = ['relu'] * max(0, L-1)\n",
    "        else:\n",
    "            activaciones = (activaciones * ((max(0, L-1) // len(activaciones)) + 1))[:max(0, L-1)]\n",
    "\n",
    "    # recorrer las capas ocultas: 1..L-1\n",
    "    for i in range(1, L):\n",
    "        # A_prev: activacion de la capa anterior (m, n_prev)\n",
    "        A_prev = cache[f'A{i-1}']\n",
    "        # W y b de la capa i\n",
    "        W = parametros[f'W{i}']\n",
    "        b = parametros[f'b{i}']\n",
    "\n",
    "        # Z = A_prev @ W + b  (producto matricial + sesgo)\n",
    "        Z = A_prev @ W + b\n",
    "\n",
    "        # obtener nombre de activacion y la funcion correspondiente\n",
    "        act_name = activaciones[i-1]\n",
    "        act_func, _ = _ACT_FUNCS.get(act_name, (relu, d_relu))\n",
    "\n",
    "        # aplicar la activacion\n",
    "        A = act_func(Z)\n",
    "\n",
    "        # guardar Z y A en cache para backprop\n",
    "        cache[f'Z{i}'] = Z\n",
    "        cache[f'A{i}'] = A\n",
    "\n",
    "    # capa de salida: usar logits ZL y aplicar softmax o sigmoide segun problema\n",
    "    W_L = parametros[f'W{L}']   # pesos de capa final\n",
    "    b_L = parametros[f'b{L}']   # bias de capa final\n",
    "    ZL = cache[f'A{L-1}'] @ W_L + b_L  # logits finales (m, n_salida)\n",
    "\n",
    "    if problema == 'multiclase':\n",
    "        # softmax para convertir logits a probabilidades por fila\n",
    "        AL = softmax(ZL)\n",
    "    else:\n",
    "        # sigmoide para salida binaria (o multiples salidas binarias)\n",
    "        AL = sigmoide(ZL)\n",
    "\n",
    "    # almacenar ZL y AL en cache\n",
    "    cache[f'Z{L}'] = ZL\n",
    "    cache[f'A{L}'] = AL\n",
    "\n",
    "    # devolver cache con todas las activaciones y valores lineales\n",
    "    return cache\n",
    "\n",
    "# -------------------------------\n",
    "# Perdida / Loss\n",
    "# -------------------------------\n",
    "\n",
    "def calcular_loss(AL, Y, problema='multiclase'):\n",
    "    # AL: salidas de la red (m, n_clases) para multiclase o (m,1) para binario\n",
    "    # Y: etiquetas (one-hot para multiclase; 0/1 para binario)\n",
    "    m = Y.shape[0]\n",
    "    eps = 1e-9  # pequeño epsilon para estabilidad en log\n",
    "\n",
    "    if problema == 'multiclase':\n",
    "        # cross-entropy: -1/m sum(Y * log(AL))\n",
    "        loss = -np.sum(Y * np.log(AL + eps)) / m\n",
    "    else:\n",
    "        # binary cross-entropy: -1/m sum(y log a + (1-y) log (1-a))\n",
    "        loss = -np.sum(Y * np.log(AL + eps) + (1 - Y) * np.log(1 - AL + eps)) / m\n",
    "\n",
    "    return loss\n",
    "\n",
    "# -------------------------------\n",
    "# Backpropagation (gradientes y actualizacion)\n",
    "# -------------------------------\n",
    "\n",
    "def backprop(cache, parametros, Y, tasa_aprendizaje=0.01, activaciones_ocultas='relu', problema='multiclase', momentum=0.0, vel=None):\n",
    "    # cache: diccionario con A0,Z1,A1,...\n",
    "    # parametros: W,b actuales\n",
    "    # Y: etiquetas correspondientes al batch\n",
    "    # tasa_aprendizaje: eta\n",
    "    # activaciones_ocultas: string o lista\n",
    "    # problema: 'multiclase' o 'binario'\n",
    "    # momentum: coef. de momentum (0 = sin momentum)\n",
    "    # vel: diccionario con velocidades previas (si usamos momentum)\n",
    "\n",
    "    m = Y.shape[0]                # tamaño del batch\n",
    "    L = len(parametros) // 2      # numero total de capas\n",
    "\n",
    "    # normalizar activaciones a lista (igual que en forward)\n",
    "    if isinstance(activaciones_ocultas, str):\n",
    "        activaciones = [activaciones_ocultas] * max(0, L-1)\n",
    "    else:\n",
    "        activaciones = list(activaciones_ocultas)\n",
    "\n",
    "    if len(activaciones) != max(0, L-1):\n",
    "        if len(activaciones) == 0:\n",
    "            activaciones = ['relu'] * max(0, L-1)\n",
    "        else:\n",
    "            activaciones = (activaciones * ((max(0, L-1) // len(activaciones)) + 1))[:max(0, L-1)]\n",
    "\n",
    "    # si usamos momentum y no hay vel proporcionada, inicializar velocidades en cero\n",
    "    if momentum and vel is None:\n",
    "        vel = {}\n",
    "        for i in range(1, L+1):\n",
    "            vel[f'W{i}'] = np.zeros_like(parametros[f'W{i}'])\n",
    "            vel[f'b{i}'] = np.zeros_like(parametros[f'b{i}'])\n",
    "\n",
    "    grads = {}  # diccionario para almacenar gradientes\n",
    "\n",
    "    # ------------------\n",
    "    # Gradiente de la capa de salida\n",
    "    # ------------------\n",
    "    AL = cache[f'A{L}']  # predicciones del batch\n",
    "\n",
    "    # para softmax+CE o sigmoid+BCE la expresion dZ = AL - Y es valida\n",
    "    dZ = AL - Y\n",
    "\n",
    "    # dW = A_{L-1}^T @ dZ / m\n",
    "    grads[f'dW{L}'] = cache[f'A{L-1}'].T @ dZ / m\n",
    "    # db = sum(dZ) / m\n",
    "    grads[f'db{L}'] = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "\n",
    "    # ------------------\n",
    "    # Retropropagar por capas ocultas en orden inverso\n",
    "    # ------------------\n",
    "    for i in range(L-1, 0, -1):\n",
    "        # dA_prev: gradiente sobre activacion de la capa anterior\n",
    "        dA_prev = dZ @ parametros[f'W{i+1}'].T\n",
    "        # recuperar Z de la capa actual para calcular derivada\n",
    "        Z_curr = cache[f'Z{i}']\n",
    "\n",
    "        # escoger derivada segun activacion\n",
    "        act_name = activaciones[i-1]\n",
    "        _, deriv = _ACT_FUNCS.get(act_name, (relu, d_relu))\n",
    "\n",
    "        # dZ = dA_prev * f'(Z)\n",
    "        dZ = dA_prev * deriv(Z_curr)\n",
    "\n",
    "        # calcular dW y db para la capa i\n",
    "        grads[f'dW{i}'] = cache[f'A{i-1}'].T @ dZ / m\n",
    "        grads[f'db{i}'] = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "\n",
    "    # ------------------\n",
    "    # Actualizar parametros: SGD simple o con momentum\n",
    "    # ------------------\n",
    "    for i in range(1, L+1):\n",
    "        if momentum and vel is not None:\n",
    "            # actualizar velocidad: v = momentum * v + (1-momentum) * grad\n",
    "            vel[f'W{i}'] = momentum * vel[f'W{i}'] + (1.0 - momentum) * grads[f'dW{i}']\n",
    "            vel[f'b{i}'] = momentum * vel[f'b{i}'] + (1.0 - momentum) * grads[f'db{i}']\n",
    "\n",
    "            # actualizar parametros usando la velocidad\n",
    "            parametros[f'W{i}'] -= tasa_aprendizaje * vel[f'W{i}']\n",
    "            parametros[f'b{i}'] -= tasa_aprendizaje * vel[f'b{i}']\n",
    "        else:\n",
    "            # SGD estandar: w = w - eta * grad\n",
    "            parametros[f'W{i}'] -= tasa_aprendizaje * grads[f'dW{i}']\n",
    "            parametros[f'b{i}'] -= tasa_aprendizaje * grads[f'db{i}']\n",
    "\n",
    "    # devolver parametros actualizados y objeto vel (para seguir usando momentum)\n",
    "    return parametros, vel\n",
    "\n",
    "# -------------------------------\n",
    "# Entrenamiento (funcion principal)\n",
    "# -------------------------------\n",
    "\n",
    "def entrenar(X, Y, capas, problema='multiclase', activaciones_ocultas='relu', metodo_init='xavier',\n",
    "             tasa_aprendizaje=0.01, epocas=1000, batch_size=None, verbose=True, seed=None, momentum=0.0):\n",
    "    # X: entradas (m, n_entradas)\n",
    "    # Y: etiquetas (one-hot para multicase, 0/1 para binario)\n",
    "    # capas: lista de tamaños por capa\n",
    "    # problema: 'multiclase' o 'binario'\n",
    "    # activaciones_ocultas: 'sigmoide'|'tanh'|'relu' o lista equivalente\n",
    "    # metodo_init: 'xavier' o 'he'\n",
    "    # tasa_aprendizaje: eta\n",
    "    # epocas: numero de iteraciones completas sobre el dataset\n",
    "    # batch_size: None -> batch completo; entero -> mini-batch\n",
    "    # verbose: imprime progreso (loss)\n",
    "    # seed: semilla para reproducibilidad\n",
    "    # momentum: coeficiente 0..<1 para usar momentum\n",
    "\n",
    "    # inicializar parametros con el metodo elegido\n",
    "    parametros = inicializar_parametros(capas, metodo_init, seed)\n",
    "    m = X.shape[0]  # numero de ejemplos\n",
    "\n",
    "    # si no se da batch_size o es invalido, usar batch completo\n",
    "    if batch_size is None or batch_size <= 0:\n",
    "        batch_size = m\n",
    "\n",
    "    vel = None  # variable para almacenar velocidades si se usa momentum\n",
    "\n",
    "    # bucle principal de entrenamiento por epocas\n",
    "    for ep in range(1, epocas + 1):\n",
    "        # permutar los datos aleatoriamente cada epoca\n",
    "        indices = np.random.permutation(m)\n",
    "        X_shuf = X[indices]\n",
    "        Y_shuf = Y[indices]\n",
    "\n",
    "        # iterar sobre mini-batches\n",
    "        for start in range(0, m, batch_size):\n",
    "            end = start + batch_size\n",
    "            X_batch = X_shuf[start:end]\n",
    "            Y_batch = Y_shuf[start:end]\n",
    "\n",
    "            # forward sobre el batch\n",
    "            cache = forward(X_batch, parametros, activaciones_ocultas, problema)\n",
    "\n",
    "            # backprop sobre el batch y actualizar parametros\n",
    "            parametros, vel = backprop(cache, parametros, Y_batch, tasa_aprendizaje, activaciones_ocultas, problema, momentum, vel)\n",
    "\n",
    "        # si verbose, calcular y mostrar loss en todo el set cada cierto numero de epocas\n",
    "        if verbose and (ep == 1 or ep % max(1, epocas // 10) == 0):\n",
    "            cache_full = forward(X, parametros, activaciones_ocultas, problema)\n",
    "            loss = calcular_loss(cache_full[f'A{len(capas)-1}'], Y, problema)\n",
    "            print(f\"Época {ep}/{epocas} - Loss: {loss:.6f}\")\n",
    "\n",
    "    # devolver parametros finales entrenados\n",
    "    return parametros\n",
    "\n",
    "# -------------------------------\n",
    "# Prediccion (uso despues de entrenar)\n",
    "# -------------------------------\n",
    "\n",
    "def predecir(X, parametros, problema='multiclase', activaciones_ocultas='relu'):\n",
    "    # realiza forward completo y devuelve clases y probabilidades\n",
    "    cache = forward(X, parametros, activaciones_ocultas, problema)  # forward\n",
    "    AL = cache[f'A{len(parametros)//2}']  # salida final\n",
    "\n",
    "    if problema == 'multiclase':\n",
    "        # devolver indice de la clase y la matriz de probabilidades\n",
    "        clases = np.argmax(AL, axis=1)\n",
    "        probs = AL\n",
    "    else:\n",
    "        # si salida es un solo valor por ejemplo, aplicar umbral 0.5\n",
    "        if AL.shape[1] == 1:\n",
    "            clases = (AL[:, 0] >= 0.5).astype(int)\n",
    "            probs = AL[:, 0:1]\n",
    "        else:\n",
    "            # multiples salidas binarias\n",
    "            clases = (AL >= 0.5).astype(int)\n",
    "            probs = AL\n",
    "\n",
    "    return clases, probs\n",
    "\n",
    "# -------------------------------\n",
    "# EJEMPLO: XOR y multiclasico sintetico\n",
    "# -------------------------------\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # -- Ejemplo XOR (problema binario) --\n",
    "    # crear entradas y salidas para XOR\n",
    "    X_xor = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=float)  # 4x2\n",
    "    Y_xor = np.array([[0],[1],[1],[0]], dtype=float)          # 4x1\n",
    "\n",
    "    # definir arquitectura: entrada 2, una oculta con 4 neuronas, salida 1\n",
    "    capas_xor = [2, 4, 1]\n",
    "\n",
    "    # entrenar la red para XOR: usando tanh en ocultas (buena para XOR), alta lr\n",
    "    parametros_xor = entrenar(X_xor, Y_xor, capas_xor, problema='binario', activaciones_ocultas='tanh',\n",
    "                              metodo_init='xavier', tasa_aprendizaje=0.5, epocas=8000, batch_size=None, verbose=True, seed=42)\n",
    "\n",
    "    # predecir y mostrar resultados para XOR\n",
    "    clases_xor, probs_xor = predecir(X_xor, parametros_xor, problema='binario', activaciones_ocultas='tanh')\n",
    "    print('XOR - Predicciones:', clases_xor)\n",
    "    print('XOR - Probabilidades:', probs_xor)\n",
    "\n",
    "    # ---- PRUEBA MANUAL XOR ----\n",
    "    nuevo = np.array([[1, 1]])   # entrada a probar\n",
    "    clase, prob = predecir(nuevo, parametros_xor, problema='binario', activaciones_ocultas='tanh')\n",
    "\n",
    "    print(\"Entrada:\", nuevo)\n",
    "    print(\"Clase predicha:\", clase)\n",
    "    print(\"Probabilidad:\", prob)\n",
    "\n",
    "    # -- Ejemplo multiclase sintetico --\n",
    "    np.random.seed(0)            # semilla para reproducibilidad\n",
    "    N = 200                     # numero de ejemplos\n",
    "    X_mc = np.random.randn(N, 2)  # ejemplo con 2 features\n",
    "    Y_idx = np.random.randint(0, 3, size=(N,))  # etiquetas 0..2\n",
    "    Y_mc = np.zeros((N, 3))      # convertimos a one-hot\n",
    "    Y_mc[np.arange(N), Y_idx] = 1\n",
    "\n",
    "    # arquitectura: 2 entradas, 2 capas ocultas, 3 salidas\n",
    "    capas_mc = [2, 16, 8, 3]\n",
    "\n",
    "    # entrenar con ReLU en la primera oculta y tanh en la segunda\n",
    "    parametros_mc = entrenar(X_mc, Y_mc, capas_mc, problema='multiclase', activaciones_ocultas=['relu','tanh'],\n",
    "                              metodo_init='he', tasa_aprendizaje=0.01, epocas=300, batch_size=32, verbose=True, seed=1)\n",
    "\n",
    "    # predecir y mostrar primeras 10 predicciones\n",
    "    clases_mc, probs_mc = predecir(X_mc, parametros_mc, problema='multiclase', activaciones_ocultas=['relu','tanh'])\n",
    "    print('Multiclase - primeros 10 predicciones:', clases_mc[:10])\n",
    "\n",
    "# FIN del archivo\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
